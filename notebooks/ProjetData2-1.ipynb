{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>pH</th>\n",
       "      <th>EC</th>\n",
       "      <th>OC</th>\n",
       "      <th>S</th>\n",
       "      <th>Zn</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mn</th>\n",
       "      <th>B</th>\n",
       "      <th>OM</th>\n",
       "      <th>Fertility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138</td>\n",
       "      <td>8.6</td>\n",
       "      <td>560</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.77</td>\n",
       "      <td>8.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.2040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>7.5</td>\n",
       "      <td>338</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.06</td>\n",
       "      <td>25.4</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1.8232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163</td>\n",
       "      <td>9.6</td>\n",
       "      <td>718</td>\n",
       "      <td>7.59</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.11</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.57</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.03</td>\n",
       "      <td>1.9092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157</td>\n",
       "      <td>6.8</td>\n",
       "      <td>475</td>\n",
       "      <td>7.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.94</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.6168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>9.9</td>\n",
       "      <td>444</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.86</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.69</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1.4792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     N    P    K    pH    EC    OC     S    Zn    Fe    Cu    Mn     B  \\\n",
       "0  138  8.6  560  7.46  0.62  0.70   5.9  0.24  0.31  0.77  8.71  0.11   \n",
       "1  213  7.5  338  7.62  0.75  1.06  25.4  0.30  0.86  1.54  2.89  2.29   \n",
       "2  163  9.6  718  7.59  0.51  1.11  14.3  0.30  0.86  1.57  2.70  2.03   \n",
       "3  157  6.8  475  7.64  0.58  0.94  26.0  0.34  0.54  1.53  2.65  1.82   \n",
       "4  270  9.9  444  7.63  0.40  0.86  11.8  0.25  0.76  1.69  2.43  2.26   \n",
       "\n",
       "       OM  Fertility  \n",
       "0  1.2040          0  \n",
       "1  1.8232          0  \n",
       "2  1.9092          0  \n",
       "3  1.6168          0  \n",
       "4  1.4792          1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Dataset2-1.csv\")\n",
    "dataset = dataset.apply(pd.to_numeric, errors='coerce')\n",
    "dataset.fillna(dataset.median(), inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop('Fertility', axis=1)\n",
    "y = dataset['Fertility']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Séparer le dataset en données d’apprentissages et données de tests (80% par classe / 20% par classe, respectivement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(708, 177)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "train_size = X_train.shape[0]\n",
    "test_size = X_test.shape[0]\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Programmer les deux algorithmes de classification “KNN ”, “Decision Trees” et “Random Forest”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_manhattan(instance1, instance2):\n",
    "    return sum(abs(a - b) for a, b in zip(instance1, instance2))\n",
    "\n",
    "def distance_euclidienne(instance1, instance2):\n",
    "    return math.sqrt(sum((a - b) ** 2 for a, b in zip(instance1, instance2)))\n",
    "\n",
    "def distance_minkowski(instance1, instance2, p):\n",
    "    return sum(abs(a - b) ** p for a, b in zip(instance1, instance2)) ** (1 / p)\n",
    "\n",
    "def distance_cosine(instance1, instance2):\n",
    "    dot_product = sum(a * b for a, b in zip(instance1, instance2))\n",
    "    magnitude1 = math.sqrt(sum(a ** 2 for a in instance1))\n",
    "    magnitude2 = math.sqrt(sum(b ** 2 for b in instance2))\n",
    "    return 1 - (dot_product / (magnitude1 * magnitude2))\n",
    "\n",
    "def distance_hamming(instance1, instance2):\n",
    "    return sum(a != b for a, b in zip(instance1, instance2))\n",
    "\n",
    "def trier_selon_distance(dataset, instance, distance_function):\n",
    "    distances = [(data, distance_function(instance, data[:-1])) for data in dataset]\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    return distances\n",
    "\n",
    "def classe_dominante(knn_instances):\n",
    "    classes = [instance[-1] for instance, _ in knn_instances] #stock les classes des instances dans une list\n",
    "    count = Counter(classes)\n",
    "    return count.most_common(1)[0][0]\n",
    "\n",
    "def predict_knn(training_set, instance, k, distance_function):\n",
    "    distances = trier_selon_distance(training_set, instance, distance_function)\n",
    "    knn = distances[:k]\n",
    "    return classe_dominante(knn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(training_set, test_x, k, distance_function):\n",
    "    predictions = []\n",
    "    for instance in test_x:\n",
    "        prediction = predict_knn(training_set, instance, k, distance_function)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(rows):\n",
    "    counts = {}\n",
    "    for row in rows:\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "def partition(rows, column, value):\n",
    "    true_rows, false_rows = [], []\n",
    "    for row in rows:\n",
    "        if row[column] >= value:\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows\n",
    "\n",
    "def gini(rows):\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity\n",
    "\n",
    "def find_best_split(rows):\n",
    "    best_gain = 0\n",
    "    best_question = None\n",
    "    current_uncertainty = gini(rows)\n",
    "    n_features = len(rows[0]) - 1\n",
    "\n",
    "    for col in range(n_features):\n",
    "        values = set([row[col] for row in rows])\n",
    "        for val in values:\n",
    "            true_rows, false_rows = partition(rows, col, val)\n",
    "\n",
    "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                continue\n",
    "\n",
    "            p = float(len(true_rows)) / (len(true_rows) + len(false_rows))\n",
    "            gain = current_uncertainty - p * gini(true_rows) - (1 - p) * gini(false_rows)\n",
    "\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_question = gain, (col, val)\n",
    "\n",
    "    return best_gain, best_question\n",
    "\n",
    "def build_tree(rows):\n",
    "    gain, question = find_best_split(rows)\n",
    "\n",
    "    if gain == 0:\n",
    "        return class_counts(rows)\n",
    "\n",
    "    true_rows, false_rows = partition(rows, question[0], question[1])\n",
    "    true_branch = build_tree(true_rows)\n",
    "    false_branch = build_tree(false_rows)\n",
    "\n",
    "    return (question, true_branch, false_branch)\n",
    "\n",
    "def classify(row, node):\n",
    "    if not isinstance(node, tuple):\n",
    "        return node\n",
    "    question, true_branch, false_branch = node\n",
    "\n",
    "    if row[question[0]] >= question[1]:\n",
    "        return classify(row, true_branch)\n",
    "    else:\n",
    "        return classify(row, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTrees(training_set, test_set):\n",
    "    tree = build_tree(training_set)\n",
    "    predictions = []\n",
    "    for instance in test_set:\n",
    "        prediction = list(classify(instance, tree).keys())[0]\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(data):\n",
    "    bootstrap = []\n",
    "    for _ in range(len(data)):\n",
    "        index = random.randint(0, len(data) - 1)\n",
    "        bootstrap.append(data[index])\n",
    "    return bootstrap\n",
    "\n",
    "def random_forest_train(data, n_trees):\n",
    "    trees = []\n",
    "    for _ in range(n_trees):\n",
    "        sample = bootstrap_sample(data)\n",
    "        tree = build_tree(sample)\n",
    "        trees.append(tree)\n",
    "    return trees\n",
    "\n",
    "def random_forest_predict(trees, row):\n",
    "    predictions = [classify(row, tree) for tree in trees]\n",
    "    final_prediction = max(predictions, key=predictions.count)\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(training_set, test_set, n_trees):\n",
    "    forest = random_forest_train(training_set, n_trees)\n",
    "    predictions = []\n",
    "    for instance in test_set:\n",
    "        prediction = list(random_forest_predict(forest, instance).keys())[0]\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Appliquer les trois algorithmes sur les instances du dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = [list(x) + [y] for x, y in zip(X_train.values, y_train.values)]\n",
    "test_set = X_test.values\n",
    "\n",
    "#KNN\n",
    "KNNpredictions = KNN(training_set, test_set, 3, distance_euclidienne)\n",
    "#DecisionTrees\n",
    "DTpredictions = DecisionTrees(training_set, test_set)\n",
    "#RandomForest\n",
    "RFpredictions = RandomForest(training_set, test_set, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Illustrer par des exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "test = [282,8.3,454,7.43,0.62,0.75,11,0.32,0.5,0.81,4.99,2.65,1.29]\n",
    "\n",
    "k=3\n",
    "n_trees=10\n",
    "\n",
    "tree = build_tree(training_set)\n",
    "forest = random_forest_train(training_set, n_trees)\n",
    "\n",
    "print(predict_knn(training_set, test, 3, distance_euclidienne))\n",
    "print(list(classify(test, tree).keys())[0])\n",
    "print(list(random_forest_predict(forest, test).keys())[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Donner la Matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrice_confusion(y_test, predictions):\n",
    "    classes = y_test.drop_duplicates()\n",
    "\n",
    "    matrice = [[0 for _ in classes] for _ in classes]\n",
    "    class_to_index = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "    for r, p in zip(y_test, predictions):\n",
    "        actual_index = class_to_index[r]\n",
    "        predicted_index = class_to_index[p]\n",
    "        matrice[actual_index][predicted_index] += 1\n",
    "    \n",
    "    return matrice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[72, 9, 0], [7, 80, 1], [0, 7, 1]],\n",
       " [[69, 12, 0], [8, 73, 7], [1, 2, 5]],\n",
       " [[70, 11, 0], [6, 75, 7], [1, 2, 5]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCKNN = matrice_confusion(y_test, KNNpredictions)\n",
    "MCDT = matrice_confusion(y_test, DTpredictions)\n",
    "MCRF = matrice_confusion(y_test, RFpredictions)\n",
    "MCKNN, MCDT, MCRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Évaluer et Comparer les modèles de classification en calculant les mesures : EXACTITUDE, SPÉCIFICITÉ, PRÉCISION, RAPPEL, F-SCORE pour chaque classe & globale en plus du temps moyen d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    correct_predictions = sum(confusion_matrix[i][i] for i in range(len(confusion_matrix)))\n",
    "    total_predictions = sum(sum(row) for row in confusion_matrix)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def specificite(confusion_matrix, classe):\n",
    "    total = sum(sum(row) for row in confusion_matrix)\n",
    "    TN_FP = total - sum(confusion_matrix[classe])\n",
    "    TN = TN_FP - sum(confusion_matrix[i][classe] for i in range(len(confusion_matrix)))\n",
    "    FP = sum(confusion_matrix[i][classe] for i in range(len(confusion_matrix))) - confusion_matrix[classe][classe]\n",
    "    return TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "def precision(confusion_matrix, classe):\n",
    "    TP = confusion_matrix[classe][classe]\n",
    "    FP = sum(confusion_matrix[i][classe] for i in range(len(confusion_matrix))) - TP\n",
    "    return TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "def rappel(confusion_matrix, classe):\n",
    "    TP = confusion_matrix[classe][classe]\n",
    "    FN = sum(confusion_matrix[classe]) - TP\n",
    "    return TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "def f_score(confusion_matrix, classe):\n",
    "    prec = precision(confusion_matrix, classe)\n",
    "    rapp = rappel(confusion_matrix, classe)\n",
    "    return 2 * (prec * rapp) / (prec + rapp) if (prec + rapp) > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Classe 0: Specificite: 0.71, Precision: 0.91, Rappel: 0.89, F-Score: 0.90\n",
      "Classe 1: Specificite: -0.78, Precision: 0.83, Rappel: 0.91, F-Score: 0.87\n",
      "Classe 2: Specificite: 0.99, Precision: 0.50, Rappel: 0.12, F-Score: 0.20\n",
      "Exactitude Globale: 0.86\n",
      "Specificite Globale: 0.31\n",
      "Precision Globale: 0.75\n",
      "Rappel Global: 0.64\n",
      "F-Score Global: 0.66\n",
      "Temps d'Execution: 1.37 secondes\n",
      "\n",
      "DecisionTrees\n",
      "Classe 0: Specificite: 0.67, Precision: 0.88, Rappel: 0.85, F-Score: 0.87\n",
      "Classe 1: Specificite: 0.12, Precision: 0.84, Rappel: 0.83, F-Score: 0.83\n",
      "Classe 2: Specificite: 0.96, Precision: 0.42, Rappel: 0.62, F-Score: 0.50\n",
      "Exactitude Globale: 0.83\n",
      "Specificite Globale: 0.58\n",
      "Precision Globale: 0.71\n",
      "Rappel Global: 0.77\n",
      "F-Score Global: 0.73\n",
      "Temps d'Execution: 2.65 secondes\n",
      "\n",
      "RandomForest\n",
      "Classe 0: Specificite: 0.54, Precision: 0.87, Rappel: 0.89, F-Score: 0.88\n",
      "Classe 1: Specificite: 0.35, Precision: 0.84, Rappel: 0.78, F-Score: 0.81\n",
      "Classe 2: Specificite: 0.95, Precision: 0.25, Rappel: 0.38, F-Score: 0.30\n",
      "Exactitude Globale: 0.81\n",
      "Specificite Globale: 0.61\n",
      "Precision Globale: 0.65\n",
      "Rappel Global: 0.68\n",
      "F-Score Global: 0.66\n",
      "Temps d'Execution: 18.58 secondes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluation(modele, training_set, test_set, y_test, **kwargs):\n",
    "    print(modele.__name__)\n",
    "    start_time = time.time()\n",
    "    predictions = modele(training_set, test_set, **kwargs)\n",
    "    end_time = time.time()\n",
    "    temps_execution = end_time - start_time\n",
    "\n",
    "    cm = matrice_confusion(y_test, predictions)\n",
    "\n",
    "    exactitude_globale = accuracy(cm)\n",
    "    specificite_globale, precision_globale, rappel_globale, f_score_globale = 0, 0, 0, 0\n",
    "    for classe in range(len(cm)):\n",
    "        spec = specificite(cm, classe)\n",
    "        prec = precision(cm, classe)\n",
    "        rapp = rappel(cm, classe)\n",
    "        fscr = f_score(cm, classe)\n",
    "        \n",
    "        specificite_globale += spec\n",
    "        precision_globale += prec\n",
    "        rappel_globale += rapp\n",
    "        f_score_globale += fscr\n",
    "        print(f\"Classe {classe}: Specificite: {format(spec, '.2f')}, Precision: {format(prec, '.2f')}, Rappel: {format(rapp, '.2f')}, F-Score: {format(fscr, '.2f')}\")\n",
    "    print(f\"Exactitude Globale: {format(exactitude_globale, '.2f')}\")\n",
    "    print(f\"Specificite Globale: {format(specificite_globale/len(cm), '.2f')}\")\n",
    "    print(f\"Precision Globale: {format(precision_globale/len(cm), '.2f')}\")\n",
    "    print(f\"Rappel Global: {format(rappel_globale/len(cm), '.2f')}\")\n",
    "    print(f\"F-Score Global: {format(f_score_globale/len(cm), '.2f')}\")\n",
    "    print(f\"Temps d'Execution: {format(temps_execution, '.2f')} secondes\")\n",
    "    print()\n",
    "\n",
    "evaluation(KNN, training_set, test_set, y_test, k=3, distance_function=distance_euclidienne)\n",
    "evaluation(DecisionTrees, training_set, test_set, y_test)\n",
    "evaluation(RandomForest, training_set, test_set, y_test, n_trees=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
